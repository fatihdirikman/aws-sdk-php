<?php
/**
 * Copyright 2010-2012 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License").
 * You may not use this file except in compliance with the License.
 * A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0
 *
 * or in the "license" file accompanying this file. This file is distributed
 * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing
 * permissions and limitations under the License.
 */

namespace Aws\Glacier\Model;

use Aws\Common\Enum\Size;
use Aws\Common\Exception\InvalidArgumentException;
use Aws\Common\Exception\OutOfBoundsException;
use Guzzle\Http\EntityBody;
use Guzzle\Http\EntityBodyInterface;

/**
 * Generates GlacierUpload objects from a string/stream that abstracts the data needed for upload requests
 */
class UploadGenerator
{
    const ALGORITHM = 'sha256';
    const SINGLE_UPLOAD = 0;

    /**
     * @var array List of cached, valid upload part sizes for validation purposes
     */
    protected static $validPartSizes;

    /**
     * @var EntityBodyInterface The body of the upload represented as a Guzzle entity body
     */
    protected $body;

    /**
     * @var array Data for the current upload object being constructed
     */
    protected $currentUpload;

    /**
     * @var array List of uploads generated by this generator
     */
    protected $generatedUploads;

    /**
     * @var array The total size of the entire upload body
     */
    protected $archiveSize;

    /**
     * @var string The tree hash of the entire upload body
     */
    protected $rootChecksum;

    /**
     * Creates a UploadGenerator and wraps the upload body in a Guzzle EntityBody object
     *
     * @param string|resource|EntityBodyInterface $body     The upload body
     * @param int                                 $partSize The size of parts to split the upload into
     *
     * @return UploadGenerator
     */
    public static function factory($body, $partSize = self::SINGLE_UPLOAD)
    {
        $body = EntityBody::factory($body);

        return new self($body, (int) $partSize);
    }

    /**
      * Takes an array of leaf nodes of a tree hash and finds the root tree hash
      *
      * @param array $hashes Array of tree hashes
      *
      * @return string
      */
    public static function calculateRootChecksum(array $hashes, $areInHexForm = false)
    {
        // Convert hex checksums into binary format before treehashing
        if ($areInHexForm) {
            $hashes = array_map(function ($hash) {
                return pack("H*" , $hash);
            }, $hashes);
        }

        // Iteratively perform hashes up the tree to arrive at a single, root tree hash
        while (count($hashes) > 1) {
            $sets = array_chunk($hashes, 2);
            $hashes = array();
            foreach ($sets as $set) {
                $hashes[] = (count($set) === 1) ? $set[0] : hash(self::ALGORITHM, $set[0] . $set[1], true);
            }
        }

        return bin2hex($hashes[0]);
    }

    /**
     * Validates a tree hash for an archive body. This is useful for validating archives downloaded from Glacier.
     *
     * @param string|array|resource|EntityBodyInterface $body            Archive body or array of part checksums
     * @param string                                    $glacierChecksum Checksum calculated by Glacier
     *
     * @return bool
     */
    public static function validateChecksum($body, $glacierChecksum)
    {
        if (is_array($body)) {
            $checksum = self::calculateRootChecksum($body, true);
        } else {
            $checksum = self::factory($body)->getRootChecksum();
        }

        return ($checksum === $glacierChecksum);

    }

    /**
     * @param EntityBodyInterface $body     The upload body
     * @param int                 $partSize The size of parts to split the upload into
     *
     * @throws InvalidArgumentException when the part size is invalid (i.e. not a power of 2 * 1MB)
     */
    public function __construct(EntityBodyInterface $body, $partSize = self::SINGLE_UPLOAD)
    {
        $this->body = $body;

        // Setup valid part sizes (1MB-4GB where 2^N MB)
        if (!self::$validPartSizes) {
            self::$validPartSizes = array_map(function ($value) {
                return pow(2, $value) * Size::MB;
            }, range(0, 12));
        }

        // Make sure the part size is valid
        if ($partSize !== self::SINGLE_UPLOAD && !in_array($partSize, self::$validPartSizes, true)) {
            throw new InvalidArgumentException('The part size must be a megabyte multiplied by a power of 2 and no'
                . 'greater than 4 gigabytes.');
        }

        $this->generateUploads($partSize);
    }

    /**
     * Returns a single upload object from the calculated uploads by index. By default it returns the first, which is
     * useful behavior if the part size was set to SINGLE_UPLOAD and there is only one upload.
     *
     * @param int $index The numerical index of the upload
     *
     * @return GlacierUpload
     * @throws OutOfBoundsException if the index of the upload doesn't exist
     */
    public function getSingleUpload($index = 0)
    {
        if (isset($this->generatedUploads[$index])) {
            return $this->generatedUploads[$index];
        } else {
            throw new OutOfBoundsException('An upload at that index did not exist.');
        }
    }

    /**
     * @return array
     */
    public function getUploads()
    {
        return $this->generatedUploads;
    }

    /**
     * @return array
     */
    public function getArchiveSize()
    {
        return $this->archiveSize;
    }

    /**
     * @return string
     */
    public function getRootChecksum()
    {
        return $this->rootChecksum;
    }

    /**
     * Performs the work of streaming through the body, creating tree hashes, and creating GlacierUpload objects
     *
     * @param int $partSize The size of parts to split the upload into
     */
    protected function generateUploads($partSize)
    {
        // Read the data from the stream and calculate hashes and sizes for uploads
        $treeHashes = array();
        $this->body->seek(0);
        $this->initializeUpload();
        while ($data = $this->body->read(Size::MB)) {
            // Add data to the upload (will add to hashes and size calculations)
            $treeHashes[] = $this->addDataToUpload($data);

            // If the upload part is complete, generate an upload object and reset the current upload
            if ($this->currentUpload['size'] === $partSize) {
                $this->generatedUploads[] = $this->createUploadObject();
                $this->initializeUpload();
            }
        }

        // Handle any leftover data
        if ($this->currentUpload['size'] > 0) {
            $this->generatedUploads[] = $this->createUploadObject();
        }

        // Calculate the root tree hash and rewind the body
        $this->rootChecksum = self::calculateRootChecksum($treeHashes);
        $this->body->seek(0);
    }

    /**
     * Initializes (or re-initializes) the data tracked in the currentUpload property
     */
    protected function initializeUpload()
    {
        $this->currentUpload = array(
            'size' => 0,
            'offset' => $this->body->ftell(),
            'hash_context' => hash_init(self::ALGORITHM),
            'tree_hashes'  => array()
        );
    }

    /**
     * Adds streamed data to an upload and contributes to the hash and length calculations
     *
     * @param string $data
     *
     * @return string The raw tree hash of the chunk
     * @throws InvalidArgumentException if the chunk of of data is not equal to 1MB or less
     */
    protected function addDataToUpload($data)
    {
        // Make sure that only 1MB chunks or smaller get passed in
        if (strlen($data) > Size::MB) {
            throw new InvalidArgumentException('The chunk of data added is too large for GlacierUploadBuilder.');
        }

        // Update the hashes and size
        $treeHash = hash(self::ALGORITHM, $data, true);
        $this->currentUpload['tree_hashes'][] = $treeHash;
        hash_update($this->currentUpload['hash_context'], $data);
        $this->currentUpload['size'] += strlen($data);

        return $treeHash;
    }

    /**
     * Creates a GlacierUpload object from the currently tracked data in the generator
     *
     * @return GlacierUpload
     */
    protected function createUploadObject()
    {
        // Calculate the tree hash of the whole upload from the tree hashes from the 1MB chunks
        $treeHash = self::calculateRootChecksum($this->currentUpload['tree_hashes']);

        // Close the streaming hash of the upload
        $contentHash = hash_final($this->currentUpload['hash_context']);

        // Get the stored size and offset and calculate the range
        $size = $this->currentUpload['size'];
        $this->archiveSize += $size;
        $offset = $this->currentUpload['offset'];
        $range = array($offset, $offset + $size - 1);

        // Return the created GlacierUpload value object
        return new GlacierUpload($treeHash, $contentHash, $size, $range, $this->body);
    }
}
